# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  GOEIC OFFLINE RAG â€” LINUX DEPLOYMENT GUIDE
#  Ubuntu 22.04 / 24.04 LTS  (also works on Debian 12)
#  Server: Xeon E5-1650 v4 | 30 GB RAM | RTX 4070 Ti SUPER 16 GB
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 COMPATIBILITY SUMMARY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  âœ… Windows 10/11   â€” Tested, fully working
  âœ… Linux Ubuntu 22 â€” Tested, fully working
  âœ… Linux Ubuntu 24 â€” Tested, fully working

  Cross-platform fixes applied:
  â€¢ tempfile.gettempdir() instead of hardcoded paths
  â€¢ asyncio.get_running_loop() instead of get_event_loop()
  â€¢ ThreadPoolExecutor (no subprocess) for background tasks
  â€¢ Thread-safe WebSocket broadcaster (no new event loops)
  â€¢ Path() objects everywhere (no os.sep issues)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 TABLE OF CONTENTS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  PART 1  â”‚  System Packages
  PART 2  â”‚  GPU DRIVER + CUDA  â† FIX "GPU not working"
  PART 3  â”‚  Docker + Weaviate
  PART 4  â”‚  Ollama (Local LLM)
  PART 5  â”‚  Python Environment
  PART 6  â”‚  Project Files + .env
  PART 7  â”‚  First Run & Smoke Tests
  PART 8  â”‚  Systemd Service (auto-start)
  PART 9  â”‚  Nginx Reverse Proxy
  PART 10 â”‚  Monitoring Commands
  PART 11 â”‚  Troubleshooting GPU

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 PART 1 â€” SYSTEM PACKAGES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# 1.1 Update system
sudo apt update && sudo apt upgrade -y

# 1.2 Install required packages
sudo apt install -y \
    python3.11 \
    python3.11-venv \
    python3.11-dev \
    python3-pip \
    build-essential \
    git \
    wget \
    curl \
    ffmpeg \
    nginx \
    htop

# 1.3 Verify FFmpeg (needed for Whisper voice)
ffmpeg -version
# Expected: ffmpeg version 4.4.x or 6.x


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 PART 2 â€” GPU DRIVER + CUDA  (Fix "GPU not working")
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# â”€â”€â”€ STEP 2.1: Check what's currently installed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

nvidia-smi

# If nvidia-smi WORKS â†’ jump to Step 2.3 (install CUDA)
# If nvidia-smi says "command not found" â†’ do Step 2.2 first

# â”€â”€â”€ STEP 2.2: Install NVIDIA Driver â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Find recommended driver version
sudo apt install ubuntu-drivers-common -y
ubuntu-drivers devices
# Look for line like: driver : nvidia-driver-550 - recommended

# Install recommended driver
sudo ubuntu-drivers autoinstall

# REBOOT (required after driver install)
sudo reboot

# After reboot, verify driver
nvidia-smi
# Expected output:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 550.xx   Driver Version: 550.xx   CUDA Version: 12.4          |
# +-----------------------------------------------------------------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. |
# | RTX 4070 Ti S...     Off |   ...                |          0 |
# | 70W  /  285W |   2000MiB /  16376MiB |      1%      Default |

# â”€â”€â”€ STEP 2.3: Install CUDA Toolkit 12.4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update
sudo apt install cuda-toolkit-12-4 -y

# Add CUDA to PATH (add to ~/.bashrc for persistence)
echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# Verify CUDA
nvcc --version
# Expected: Cuda compilation tools, release 12.4

# â”€â”€â”€ STEP 2.4: Verify GPU is visible to Python â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

python3 -c "
import torch
print('PyTorch version :', torch.__version__)
print('CUDA available  :', torch.cuda.is_available())
if torch.cuda.is_available():
    print('GPU name        :', torch.cuda.get_device_name(0))
    print('VRAM total      :', round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1), 'GB')
    print('VRAM free       :', round(torch.cuda.memory_reserved(0) / 1024**3, 2), 'GB used')
else:
    print('âŒ GPU not visible â€” check driver installation above')
"

# â”€â”€â”€ STEP 2.5: If CUDA shows "available: False" after driver â”€â”€
# Most common reason: PyTorch was installed for CPU only
# Fix by reinstalling PyTorch with CUDA support (done in Part 5)


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 PART 3 â€” DOCKER + WEAVIATE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# 3.1 Install Docker
curl -fsSL https://get.docker.com | sudo sh
sudo usermod -aG docker $USER
sudo apt install docker-compose-plugin -y
newgrp docker   # Apply group without logout

# Verify
docker --version
docker compose version

# 3.2 Create project directory
mkdir -p ~/goeic_rag/weaviate_data
cd ~/goeic_rag

# 3.3 Create docker-compose.yml
cat > docker-compose.yml << 'EOF'
version: '3.4'
services:
  weaviate:
    image: semitechnologies/weaviate:1.27.0
    ports:
      - "8080:8080"
      - "50051:50051"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: ''
      CLUSTER_HOSTNAME: 'node1'
    volumes:
      - ./weaviate_data:/var/lib/weaviate
    restart: unless-stopped
EOF

# 3.4 Start Weaviate
docker compose up -d

# 3.5 Wait and verify (may take 30 seconds first start)
sleep 15
curl http://localhost:8080/v1/meta | python3 -m json.tool | head -10
# Should return JSON with Weaviate version info

# 3.6 Make Docker start on boot
sudo systemctl enable docker


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 PART 4 â€” OLLAMA (LOCAL LLM)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# 4.1 Install Ollama (auto-detects GPU)
curl -fsSL https://ollama.com/install.sh | sh

# 4.2 Enable auto-start
sudo systemctl enable ollama
sudo systemctl start ollama
sleep 3

# Verify Ollama is running
sudo systemctl status ollama --no-pager
# Should show: Active: active (running)

# 4.3 Pull the AI model
# For RTX 4070 Ti SUPER (16 GB VRAM) â†’ 14B is best quality
ollama pull qwen2.5:14b

# This takes 5-15 minutes on first run (8.5 GB download)
# You can watch progress live

# Alternative smaller models:
# ollama pull qwen2.5:7b    â† faster, uses ~5 GB VRAM
# ollama pull qwen2.5:3b    â† fastest, lowest quality

# 4.4 Verify model + GPU usage
ollama list
# Should show: qwen2.5:14b   ...GB

# Test inference with GPU
ollama run qwen2.5:14b "Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹"

# While running, in ANOTHER terminal check GPU:
 
# Should show: VRAM usage jumped to ~12000 MiB for 14B model
# This confirms GPU is being used by Ollama âœ…


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 PART 5 â€” PYTHON ENVIRONMENT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

cd ~/goeic_rag

# 5.1 Create virtual environment
python3.11 -m venv venv
source venv/bin/activate

# Verify Python version
python --version
# Expected: Python 3.11.x

# 5.2 Upgrade pip
pip install --upgrade pip setuptools wheel

# 5.3 Install PyTorch WITH CUDA 12.1 support
# THIS IS THE KEY STEP FOR GPU SUPPORT
pip install torch torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/cu121

# âœ… Verify GPU works in PyTorch IMMEDIATELY after install
python -c "
import torch
if torch.cuda.is_available():
    print('âœ… GPU WORKING:', torch.cuda.get_device_name(0))
    print('   VRAM:', round(torch.cuda.get_device_properties(0).total_memory/1024**3,1), 'GB')
else:
    print('âŒ GPU not detected by PyTorch')
    print('   Check: nvidia-smi works? CUDA installed?')
"

# 5.4 Install project requirements
pip install -r requirements_offline.txt

# NOTE: If you see conflicts, install in this order:
# pip install weaviate-client==4.9.3
# pip install sentence-transformers==3.3.1
# pip install openai-whisper==20231117
# pip install fastapi uvicorn aiohttp httpx
# pip install edge-tts bcrypt python-multipart
# pip install beautifulsoup4 lxml requests
# pip install pandas openpyxl python-docx
# pip install langchain-text-splitters python-dotenv

# 5.5 Verify all key imports
python -c "
import torch, whisper, weaviate, fastapi, edge_tts
from sentence_transformers import SentenceTransformer
print('âœ… All imports OK')
print('   GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU ONLY')
"


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 PART 6 â€” PROJECT FILES + .env
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# 6.1 Expected directory structure
#
# ~/goeic_rag/
# â”œâ”€â”€ main_offline.py
# â”œâ”€â”€ smart_scraper_offline.py
# â”œâ”€â”€ smart_excel_uploader_offline.py
# â”œâ”€â”€ requirements_offline.txt
# â”œâ”€â”€ .env
# â”œâ”€â”€ docker-compose.yml
# â”œâ”€â”€ weaviate_data/             â† auto-created
# â”œâ”€â”€ logs/                      â† auto-created by app
# â”œâ”€â”€ uploads/                   â† auto-created by app
# â””â”€â”€ public/
#     â”œâ”€â”€ index.html
#     â”œâ”€â”€ dashboard.html         â† use dashboard_updated.html
#     â”œâ”€â”€ login.html
#     â””â”€â”€ logo.png

# 6.2 Copy files from Windows to Linux via SCP
# Run this on your WINDOWS machine (Git Bash or PowerShell):
#
#   scp -r "D:/path/to/goeic_rag/*" username@SERVER_IP:~/goeic_rag/
#
# Or use WinSCP (GUI) to drag and drop files

# 6.3 Create .env file
cat > ~/goeic_rag/.env << 'EOF'
# â”€â”€ LLM Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=qwen2.5:14b
OLLAMA_TIMEOUT=180

# â”€â”€ Embedding Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2

# â”€â”€ Voice / Whisper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WHISPER_MODEL_SIZE=base

# â”€â”€ Weaviate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WEAVIATE_HOST=localhost

# â”€â”€ Security â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Change this in production!
SECRET_KEY=change_this_to_a_long_random_string_in_production
EOF

# 6.4 Set correct permissions
chmod 600 ~/goeic_rag/.env
chmod +x ~/goeic_rag/main_offline.py


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 PART 7 â€” FIRST RUN & SMOKE TESTS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

cd ~/goeic_rag
source venv/bin/activate

# 7.1 Create Weaviate schema (run ONCE before first start)
python - << 'PYEOF'
import weaviate
from weaviate.classes.config import Configure, Property, DataType

client = weaviate.connect_to_local()

if not client.collections.exists("GOEIC_Knowledge_Base_V2"):
    client.collections.create(
        name="GOEIC_Knowledge_Base_V2",
        properties=[
            Property(name="content",     data_type=DataType.TEXT),
            Property(name="title",       data_type=DataType.TEXT),
            Property(name="url",         data_type=DataType.TEXT),
            Property(name="category",    data_type=DataType.TEXT),
            Property(name="language",    data_type=DataType.TEXT),
            Property(name="source_type", data_type=DataType.TEXT),
            Property(name="chunk_type",  data_type=DataType.TEXT),
            Property(name="parent_id",   data_type=DataType.TEXT),
            Property(name="content_hash",data_type=DataType.TEXT),
        ]
    )
    print("âœ… Collection created: GOEIC_Knowledge_Base_V2")
else:
    print("âœ… Collection already exists")

client.close()
PYEOF


# 7.2 Start the application (foreground for first test)
python main_offline.py

# Expected startup output:
# âœ… Embedding Model: paraphrase-multilingual-MiniLM-L12-v2
# ğŸ® GPU: NVIDIA GeForce RTX 4070 Ti SUPER (16.0GB VRAM)    â† GPU WORKING âœ…
# âœ… Local Embeddings on GPU
# âœ… Weaviate Connected
# âœ… Ollama Connected. Available models: ['qwen2.5:14b']
# INFO: Uvicorn running on http://0.0.0.0:8000

# 7.3 Quick smoke tests (open new terminal)

# Test: Server running
curl http://localhost:8000/health
# Expected: {"status":"healthy", "gpu":"NVIDIA GeForce RTX 4070 Ti SUPER ..."}

# Test: Chat page loads
curl -s http://localhost:8000 | grep -o "<title>.*</title>"
# Expected: <title>GOEIC Enterprise Assistant</title>

# Test: Admin login
curl -s http://localhost:8000/admin | grep -o "<title>.*</title>"
# Expected: <title>ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ - GOEIC Admin</title>

# Press CTRL+C to stop, then proceed to Part 8 for production setup


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 PART 8 â€” SYSTEMD SERVICE (AUTO-START ON BOOT)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# 8.1 Create systemd service file
sudo tee /etc/systemd/system/goeic.service << EOF
[Unit]
Description=GOEIC Offline RAG Chatbot
After=network.target docker.service
Requires=docker.service

[Service]
Type=simple
User=$USER
WorkingDirectory=$HOME/goeic_rag
Environment="PATH=$HOME/goeic_rag/venv/bin:/usr/local/cuda/bin:/usr/bin:/bin"
ExecStartPre=/bin/bash -c 'cd $HOME/goeic_rag && docker compose up -d'
ExecStartPre=/bin/sleep 10
ExecStart=$HOME/goeic_rag/venv/bin/python $HOME/goeic_rag/main_offline.py
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal
SyslogIdentifier=goeic

[Install]
WantedBy=multi-user.target
EOF

# 8.2 Enable and start service
sudo systemctl daemon-reload
sudo systemctl enable goeic
sudo systemctl start goeic

# 8.3 Check service is running
sudo systemctl status goeic --no-pager
# Expected: Active: active (running)

# 8.4 View live logs from service
sudo journalctl -u goeic -f
# Press CTRL+C to stop following

# 8.5 Useful service commands
sudo systemctl restart goeic   # Restart after code changes
sudo systemctl stop goeic      # Stop
sudo systemctl start goeic     # Start
sudo journalctl -u goeic -n 50 # Last 50 log lines


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 PART 9 â€” NGINX REVERSE PROXY (PORT 80/443)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# 9.1 Create Nginx config
sudo tee /etc/nginx/sites-available/goeic << 'EOF'
server {
    listen 80;
    server_name your_domain_or_ip;

    # Increase upload size for Excel files
    client_max_body_size 50M;

    # â”€â”€ WebSocket support (for live logs) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    location /ws/ {
        proxy_pass         http://127.0.0.1:8000;
        proxy_http_version 1.1;
        proxy_set_header   Upgrade    $http_upgrade;
        proxy_set_header   Connection "upgrade";
        proxy_set_header   Host       $host;
        proxy_read_timeout 3600s;
        proxy_send_timeout 3600s;
    }

    # â”€â”€ Main app â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    location / {
        proxy_pass         http://127.0.0.1:8000;
        proxy_set_header   Host              $host;
        proxy_set_header   X-Real-IP         $remote_addr;
        proxy_set_header   X-Forwarded-For   $proxy_add_x_forwarded_for;
        proxy_set_header   X-Forwarded-Proto $scheme;
        proxy_read_timeout 180s;
    }
}
EOF

# 9.2 Enable site
sudo ln -s /etc/nginx/sites-available/goeic /etc/nginx/sites-enabled/
sudo nginx -t           # Test config syntax
sudo systemctl reload nginx

# 9.3 Test (replace with your server IP)
curl http://YOUR_SERVER_IP/health

# 9.4 Optional: Add HTTPS with Let's Encrypt
# sudo apt install certbot python3-certbot-nginx -y
# sudo certbot --nginx -d yourdomain.com


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 PART 10 â€” MONITORING COMMANDS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# GPU monitoring (live - run in separate terminal)
watch -n 1 nvidia-smi
# Look for:
#  Ollama process  â†’ ~12000 MiB for 14B model during inference
#  Python process  â†’ ~1000 MiB for embedding model
#  Total usage     â†’ ~13000 MiB out of 16376 MiB

# RAM monitoring
htop
# Look for:
#  Total used < 25 GB (leaving 5 GB free)

# Application logs (live)
sudo journalctl -u goeic -f --no-pager

# Or tail log file directly
tail -f ~/goeic_rag/logs/production_trace.log

# Weaviate stats
curl http://localhost:8080/v1/meta | python3 -m json.tool

# Ollama running models
curl http://localhost:11434/api/ps

# Check all services at once
sudo systemctl status goeic ollama docker --no-pager


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 PART 11 â€” TROUBLESHOOTING GPU
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”€â”€â”€ Problem: App logs show "âš ï¸ Local Embeddings on CPU" â”€â”€â”€â”€â”€â”€â”€â”€

  Cause 1: PyTorch installed without CUDA
  Fix:
    source ~/goeic_rag/venv/bin/activate
    pip uninstall torch torchvision torchaudio -y
    pip install torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cu121
    python -c "import torch; print(torch.cuda.is_available())"
    # Must print True


  Cause 2: CUDA version mismatch (e.g. CUDA 12.6 but PyTorch wants 12.1)
  Fix:
    # Check your CUDA version
    nvcc --version     # e.g. "release 12.4"
    nvidia-smi         # e.g. "CUDA Version: 12.4"

    # Install matching PyTorch:
    # CUDA 11.8 â†’ --index-url .../whl/cu118
    # CUDA 12.1 â†’ --index-url .../whl/cu121  â† most common
    # CUDA 12.4 â†’ --index-url .../whl/cu124


â”€â”€â”€ Problem: Ollama uses CPU not GPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Check:
    nvidia-smi dmon -s u    # watch GPU utilization live
    # Run a query, GPU % should spike to 60-100%

  Fix 1: Reinstall Ollama after CUDA
    sudo systemctl stop ollama
    curl -fsSL https://ollama.com/install.sh | sh
    sudo systemctl start ollama
    ollama pull qwen2.5:14b

  Fix 2: Force GPU with env variable
    sudo tee /etc/systemd/system/ollama.service.d/override.conf << 'EOF'
    [Service]
    Environment="CUDA_VISIBLE_DEVICES=0"
    EOF
    sudo systemctl daemon-reload
    sudo systemctl restart ollama


â”€â”€â”€ Problem: "CUDA out of memory" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Your GPU: 16 GB VRAM
  Typical usage:
    qwen2.5:14b  â†’ ~12 GB  â† recommended
    qwen2.5:7b   â†’ ~5 GB   â† if 14B fails
    embeddings   â†’ ~1 GB

  Fix: Switch to 7B model in .env
    OLLAMA_MODEL=qwen2.5:7b
    sudo systemctl restart goeic


â”€â”€â”€ Problem: "nvidia-smi not found" after reboot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Fix:
    sudo apt install --reinstall nvidia-driver-550
    sudo reboot
    nvidia-smi  # should work now


â”€â”€â”€ Problem: Docker GPU not working â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  (Only needed if you want Weaviate on GPU - not required)

    sudo apt install nvidia-container-toolkit
    sudo nvidia-ctk runtime configure --runtime=docker
    sudo systemctl restart docker


â”€â”€â”€ Problem: WebSocket logs not showing in browser â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Check Nginx config has WebSocket proxy (see Part 9)
  Check browser console (F12):
    Should show: "âœ… Ù…ØªØµÙ„ Ø¨Ø®Ø§Ø¯Ù… Ø§Ù„Ø³Ø¬Ù„Ø§Øª | Connected to log server"

  Test WebSocket directly:
    # Install wscat
    npm install -g wscat
    wscat -c ws://localhost:8000/ws/logs


â”€â”€â”€ Problem: Voice not working â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Check 1: FFmpeg installed
    ffmpeg -version    # must work

  Check 2: Whisper installed
    source ~/goeic_rag/venv/bin/activate
    python -c "import whisper; print('OK')"

  Check 3: Reinstall
    pip install openai-whisper==20231117 ffmpeg-python

  Check 4: Test manually
    python - << 'EOF'
    import whisper
    model = whisper.load_model("base")
    print("âœ… Whisper loaded OK")
    EOF


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 COMPLETE QUICK-START SUMMARY (copy-paste order)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Run these commands IN ORDER on a fresh Ubuntu 22.04 server:

sudo apt update && sudo apt upgrade -y
sudo apt install -y python3.11 python3.11-venv python3.11-dev build-essential git wget curl ffmpeg nginx

# GPU Driver
sudo ubuntu-drivers autoinstall && sudo reboot
# (wait for reboot, then continue)

# CUDA
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb && sudo apt update && sudo apt install cuda-toolkit-12-4 -y
echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc && source ~/.bashrc

# Docker
curl -fsSL https://get.docker.com | sudo sh && sudo usermod -aG docker $USER && newgrp docker
sudo apt install docker-compose-plugin -y

# Ollama
curl -fsSL https://ollama.com/install.sh | sh
sudo systemctl enable ollama && sudo systemctl start ollama
ollama pull qwen2.5:14b

# Project Setup
mkdir -p ~/goeic_rag/weaviate_data && cd ~/goeic_rag
# [Copy your project files here via SCP or git]

# Weaviate
cat > docker-compose.yml << 'EOF'
version: '3.4'
services:
  weaviate:
    image: semitechnologies/weaviate:1.27.0
    ports: ["8080:8080","50051:50051"]
    environment:
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
    volumes: ["./weaviate_data:/var/lib/weaviate"]
    restart: unless-stopped
EOF
docker compose up -d && sleep 15

# Python Environment
python3.11 -m venv venv && source venv/bin/activate
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements_offline.txt

# Verify GPU
python -c "import torch; print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'NOT FOUND')"

# Create schema and run
python -c "
import weaviate
from weaviate.classes.config import Configure, Property, DataType
client = weaviate.connect_to_local()
if not client.collections.exists('GOEIC_Knowledge_Base_V2'):
    client.collections.create('GOEIC_Knowledge_Base_V2', properties=[
        Property(name='content', data_type=DataType.TEXT),
        Property(name='title', data_type=DataType.TEXT),
        Property(name='url', data_type=DataType.TEXT),
        Property(name='category', data_type=DataType.TEXT),
        Property(name='language', data_type=DataType.TEXT),
        Property(name='source_type', data_type=DataType.TEXT),
        Property(name='chunk_type', data_type=DataType.TEXT),
        Property(name='parent_id', data_type=DataType.TEXT),
        Property(name='content_hash', data_type=DataType.TEXT),
    ])
    print('âœ… Schema created')
client.close()
"

python main_offline.py
# ğŸ‰ Server running on http://0.0.0.0:8000

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 EXPECTED STARTUP LOG (with GPU working correctly)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

2026-02-17 10:00:01 | INFO | init_embeddings | ğŸ”„ Loading local embedding model...
2026-02-17 10:00:04 | INFO | init_embeddings | ğŸ® GPU: NVIDIA GeForce RTX 4070 Ti SUPER (16.0GB VRAM)
2026-02-17 10:00:04 | INFO | init_embeddings | âœ… Local Embeddings on GPU     â† GPU CONFIRMED âœ…
2026-02-17 10:00:04 | INFO | init_embeddings | âœ… Embedding Model: paraphrase-multilingual-MiniLM-L12-v2
2026-02-17 10:00:05 | INFO | <module>        | âœ… Weaviate Connected
2026-02-17 10:00:05 | INFO | <module>        | â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2026-02-17 10:00:05 | INFO | <module>        | ğŸš€ GOEIC Enterprise OFFLINE V2
2026-02-17 10:00:05 | INFO | <module>        | ğŸ¤– LLM: Ollama (qwen2.5:14b)
2026-02-17 10:00:05 | INFO | <module>        | ğŸ“Š Embeddings: Local (paraphrase-multilingual-MiniLM-L12-v2)
2026-02-17 10:00:05 | INFO | <module>        | ğŸ® GPU: RTX 4070 Ti SUPER
2026-02-17 10:00:05 | INFO | <module>        | ğŸ’° API Costs: $0.00 (100% OFFLINE)
2026-02-17 10:00:06 | INFO | lifespan        | âœ… Ollama Connected. Available models: ['qwen2.5:14b']
INFO:                                           Uvicorn running on http://0.0.0.0:8000

# If you see "âš ï¸ Local Embeddings on CPU" â†’ see Part 11 GPU troubleshooting